<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Real-time Audio Recording - Depression Detection</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            min-height: 100vh;
        }

        .container {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 20px;
            padding: 30px;
            backdrop-filter: blur(10px);
            box-shadow: 0 20px 40px rgba(0,0,0,0.3);
        }

        h1 {
            text-align: center;
            margin-bottom: 30px;
            font-size: 2.5em;
            color: #ecf0f1;
        }

        .warning-box {
            background: rgba(231, 76, 60, 0.2);
            border: 2px solid #e74c3c;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            color: #ecf0f1;
        }

        .info-box {
            background: rgba(52, 152, 219, 0.2);
            border: 2px solid #3498db;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            color: #ecf0f1;
        }

        .recording-section {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            text-align: center;
        }

        .record-btn {
            padding: 15px 40px;
            font-size: 18px;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: bold;
            margin: 10px;
        }

        .record-btn.start {
            background: linear-gradient(45deg, #27ae60, #2ecc71);
            color: white;
        }

        .record-btn.stop {
            background: linear-gradient(45deg, #e74c3c, #c0392b);
            color: white;
            animation: pulse 1.5s infinite;
        }

        .record-btn:disabled {
            background: #7f8c8d;
            cursor: not-allowed;
        }

        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 0 0 0 rgba(231, 76, 60, 0.7); }
            50% { transform: scale(1.05); box-shadow: 0 0 0 10px rgba(231, 76, 60, 0); }
            100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(231, 76, 60, 0); }
        }

        .status-display {
            padding: 15px;
            border-radius: 10px;
            margin: 15px 0;
            font-weight: bold;
            text-align: center;
        }

        .status-idle { background: rgba(52, 152, 219, 0.3); }
        .status-recording { background: rgba(231, 76, 60, 0.3); }
        .status-processing { background: rgba(243, 156, 18, 0.3); }
        .status-complete { background: rgba(39, 174, 96, 0.3); }

        .audio-visualizer {
            width: 100%;
            height: 120px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 10px;
            margin: 20px 0;
            border: 2px solid rgba(255, 255, 255, 0.2);
        }

        .transcript-display {
            width: 100%;
            min-height: 150px;
            padding: 20px;
            border: 2px solid rgba(255, 255, 255, 0.2);
            border-radius: 10px;
            background: rgba(0, 0, 0, 0.3);
            color: #ecf0f1;
            font-size: 16px;
            resize: vertical;
            font-family: 'Courier New', monospace;
        }

        .code-section {
            background: rgba(0, 0, 0, 0.4);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }

        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.5;
        }

        .implementation-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-card {
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #3498db;
        }

        .audio-controls {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin: 20px 0;
        }

        .volume-meter {
            width: 200px;
            height: 10px;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 5px;
            overflow: hidden;
        }

        .volume-fill {
            height: 100%;
            background: linear-gradient(90deg, #27ae60, #f39c12, #e74c3c);
            width: 0%;
            transition: width 0.1s ease;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Real-time Audio Recording System</h1>
        
        <div class="warning-box">
            <h3>‚ö†Ô∏è Security Note</h3>
            <p><strong>API Key Protection:</strong> Your ElevenLabs API key should be stored server-side, not in client-side code. This demo shows the frontend implementation - you'll need a backend endpoint to securely handle the API calls.</p>
        </div>

        <div class="info-box">
            <h3>üîÑ PyAudio ‚Üí Web Audio API Translation</h3>
            <p><strong>This implementation replaces PyAudio functionality with Web APIs:</strong></p>
            <ul>
                <li><strong>PyAudio.open() ‚Üí navigator.mediaDevices.getUserMedia()</strong></li>
                <li><strong>stream.read() ‚Üí MediaRecorder.ondataavailable</strong></li>
                <li><strong>PyAudio analysis ‚Üí Web Audio API AnalyserNode</strong></li>
            </ul>
        </div>

        <div class="recording-section">
            <div class="status-display status-idle" id="statusDisplay">
                Ready to Record
            </div>
            
            <div class="audio-controls">
                <button class="record-btn start" id="recordButton">
                    üéôÔ∏è Start Recording
                </button>
                
                <div class="volume-meter">
                    <div class="volume-fill" id="volumeFill"></div>
                </div>
            </div>
            
            <canvas class="audio-visualizer" id="audioVisualizer"></canvas>
            
            <textarea class="transcript-display" id="transcriptDisplay" 
                      placeholder="Transcript will appear here after recording stops..." readonly></textarea>
        </div>

        <div class="code-section">
            <h2>üìù Implementation Details</h2>
            
            <div class="implementation-grid">
                <div class="comparison-card">
                    <h3>üêç PyAudio (Python)</h3>
                    <div class="code-block">
<pre>import pyaudio

# Initialize PyAudio
p = pyaudio.PyAudio()

# Open stream
stream = p.open(
    format=pyaudio.paInt16,
    channels=1,
    rate=44100,
    input=True,
    frames_per_buffer=1024
)

# Read audio data
data = stream.read(1024)</pre>
                    </div>
                </div>
                
                <div class="comparison-card">
                    <h3>üåê Web Audio API (JavaScript)</h3>
                    <div class="code-block">
<pre>// Get microphone access
const stream = await navigator
  .mediaDevices.getUserMedia({
    audio: {
      sampleRate: 44100,
      channelCount: 1,
      echoCancellation: true
    }
  });

// Create audio context
const audioContext = new AudioContext();
const source = audioContext
  .createMediaStreamSource(stream);</pre>
                    </div>
                </div>
            </div>

            <h3>üîß Complete Web Audio Implementation</h3>
            <div class="code-block">
<pre>class AudioRecorder {
    constructor() {
        this.mediaRecorder = null;
        this.audioContext = null;
        this.analyser = null;
        this.isRecording = false;
        this.audioChunks = [];
    }
    
    async startRecording() {
        // Get microphone stream (replaces PyAudio.open())
        const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                sampleRate: 44100,
                channelCount: 1
            }
        });
        
        // Create audio context for analysis (replaces PyAudio analysis)
        this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        this.analyser = this.audioContext.createAnalyser();
        const source = this.audioContext.createMediaStreamSource(stream);
        source.connect(this.analyser);
        
        // Setup for frequency analysis
        this.analyser.fftSize = 2048;
        this.analyser.smoothingTimeConstant = 0.8;
        
        // Setup recorder (replaces stream.read())
        this.mediaRecorder = new MediaRecorder(stream, {
            mimeType: 'audio/webm;codecs=opus'
        });
        
        this.audioChunks = [];
        
        this.mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                this.audioChunks.push(event.data);
            }
        };
        
        this.mediaRecorder.onstop = () => {
            this.processAudioData();
        };
        
        this.mediaRecorder.start(100); // Collect every 100ms
        this.isRecording = true;
        
        // Start real-time analysis
        this.startAudioAnalysis();
    }
    
    startAudioAnalysis() {
        const bufferLength = this.analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        const timeDataArray = new Float32Array(bufferLength);
        
        const analyze = () => {
            if (!this.isRecording) return;
            
            // Get frequency data (replaces PyAudio frequency analysis)
            this.analyser.getByteFrequencyData(dataArray);
            this.analyser.getFloatTimeDomainData(timeDataArray);
            
            // Calculate volume level
            let sum = 0;
            for (let i = 0; i < timeDataArray.length; i++) {
                sum += timeDataArray[i] * timeDataArray[i];
            }
            const volume = Math.sqrt(sum / timeDataArray.length);
            
            // Update UI with real-time data
            this.updateVolumeDisplay(volume);
            this.updateVisualizer(dataArray);
            
            requestAnimationFrame(analyze);
        };
        
        analyze();
    }
}</pre>
            </div>
        </div>
    </div>

    <script>
        class RealTimeAudioRecorder {
            constructor() {
                this.mediaRecorder = null;
                this.audioContext = null;
                this.analyser = null;
                this.isRecording = false;
                this.audioChunks = [];
                this.canvas = null;
                this.canvasContext = null;
                
                this.initializeElements();
                this.setupCanvas();
            }
            
            initializeElements() {
                this.recordButton = document.getElementById('recordButton');
                this.statusDisplay = document.getElementById('statusDisplay');
                this.transcriptDisplay = document.getElementById('transcriptDisplay');
                this.volumeFill = document.getElementById('volumeFill');
                
                this.recordButton.addEventListener('click', () => this.toggleRecording());
            }
            
            setupCanvas() {
                this.canvas = document.getElementById('audioVisualizer');
                this.canvasContext = this.canvas.getContext('2d');
                this.canvas.width = this.canvas.offsetWidth;
                this.canvas.height = this.canvas.offsetHeight;
            }
            
            async toggleRecording() {
                if (!this.isRecording) {
                    await this.startRecording();
                } else {
                    this.stopRecording();
                }
            }
            
            async startRecording() {
                try {
                    // Web Audio API replaces PyAudio stream opening
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 44100,
                            channelCount: 1
                        }
                    });
                    
                    // Create audio context for real-time analysis (replaces PyAudio analysis)
                    this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    this.analyser = this.audioContext.createAnalyser();
                    const source = this.audioContext.createMediaStreamSource(stream);
                    source.connect(this.analyser);
                    
                    // Configure analyser (equivalent to PyAudio buffer settings)
                    this.analyser.fftSize = 2048;
                    this.analyser.smoothingTimeConstant = 0.8;
                    
                    // Setup MediaRecorder (replaces PyAudio stream.read())
                    this.mediaRecorder = new MediaRecorder(stream, {
                        mimeType: this.getSupportedMimeType()
                    });
                    
                    this.audioChunks = [];
                    
                    this.mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0) {
                            this.audioChunks.push(event.data);
                        }
                    };
                    
                    this.mediaRecorder.onstop = () => {
                        this.processRecordedAudio();
                    };
                    
                    this.mediaRecorder.start(100); // Collect data every 100ms
                    this.isRecording = true;
                    
                    this.updateStatus('recording', 'Recording in progress...');
                    this.recordButton.className = 'record-btn stop';
                    this.recordButton.textContent = 'üî¥ Stop Recording';
                    
                    // Start real-time audio analysis (replaces PyAudio real-time processing)
                    this.startRealtimeAnalysis();
                    
                } catch (error) {
                    console.error('Error accessing microphone:', error);
                    this.updateStatus('idle', 'Error: Could not access microphone');
                }
            }
            
            getSupportedMimeType() {
                const types = [
                    'audio/webm;codecs=opus',
                    'audio/webm',
                    'audio/mp4',
                    'audio/wav'
                ];
                
                for (const type of types) {
                    if (MediaRecorder.isTypeSupported(type)) {
                        return type;
                    }
                }
                
                return 'audio/webm'; // fallback
            }
            
            startRealtimeAnalysis() {
                const bufferLength = this.analyser.frequencyBinCount;
                const frequencyData = new Uint8Array(bufferLength);
                const timeDomainData = new Float32Array(bufferLength);
                
                const analyze = () => {
                    if (!this.isRecording) return;
                    
                    // Get frequency and time domain data (replaces PyAudio data analysis)
                    this.analyser.getByteFrequencyData(frequencyData);
                    this.analyser.getFloatTimeDomainData(timeDomainData);
                    
                    // Calculate RMS volume (equivalent to PyAudio volume calculation)
                    let sum = 0;
                    for (let i = 0; i < timeDomainData.length; i++) {
                        sum += timeDomainData[i] * timeDomainData[i];
                    }
                    const rmsVolume = Math.sqrt(sum / timeDomainData.length);
                    
                    // Update real-time displays
                    this.updateVolumeIndicator(rmsVolume);
                    this.updateFrequencyVisualizer(frequencyData);
                    
                    // Continue analysis loop
                    requestAnimationFrame(analyze);
                };
                
                analyze();
            }
            
            updateVolumeIndicator(volume) {
                // Convert to percentage and apply to volume meter
                const percentage = Math.min(100, volume * 1000);
                this.volumeFill.style.width = `${percentage}%`;
            }
            
            updateFrequencyVisualizer(frequencyData) {
                // Clear canvas
                this.canvasContext.fillStyle = 'rgba(44, 62, 80, 0.8)';
                this.canvasContext.fillRect(0, 0, this.canvas.width, this.canvas.height);
                
                // Draw frequency bars
                const barWidth = (this.canvas.width / frequencyData.length) * 2.5;
                let x = 0;
                
                for (let i = 0; i < frequencyData.length; i++) {
                    const barHeight = (frequencyData[i] / 255) * this.canvas.height * 0.8;
                    
                    // Create gradient for bars
                    const gradient = this.canvasContext.createLinearGradient(0, this.canvas.height, 0, 0);
                    gradient.addColorStop(0, '#27ae60');
                    gradient.addColorStop(0.5, '#f39c12');
                    gradient.addColorStop(1, '#e74c3c');
                    
                    this.canvasContext.fillStyle = gradient;
                    this.canvasContext.fillRect(x, this.canvas.height - barHeight, barWidth, barHeight);
                    
                    x += barWidth + 1;
                }
            }
            
            stopRecording() {
                if (this.mediaRecorder && this.isRecording) {
                    this.mediaRecorder.stop();
                    this.isRecording = false;
                    
                    // Stop all audio tracks
                    this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                    
                    if (this.audioContext) {
                        this.audioContext.close();
                    }
                    
                    this.updateStatus('processing', 'Processing recorded audio...');
                    this.recordButton.className = 'record-btn start';
                    this.recordButton.textContent = 'üéôÔ∏è Start Recording';
                    this.recordButton.disabled = true;
                }
            }
            
            async processRecordedAudio() {
                try {
                    // Convert recorded chunks to blob
                    const audioBlob = new Blob(this.audioChunks, { 
                        type: this.mediaRecorder.mimeType 
                    });
                    
                    // Convert to WAV for better compatibility
                    const wavBlob = await this.convertToWAV(audioBlob);
                    
                    // Send to your backend endpoint (keeps API key secure)
                    const transcript = await this.sendToTranscriptionService(wavBlob);
                    
                    // Display result
                    this.transcriptDisplay.value = transcript;
                    this.updateStatus('complete', 'Transcription completed successfully!');
                    
                    // Analyze for depression indicators (your ML logic here)
                    this.analyzeTranscript(transcript);
                    
                } catch (error) {
                    console.error('Processing error:', error);
                    this.updateStatus('idle', 'Error processing audio. Please try again.');
                } finally {
                    this.recordButton.disabled = false;
                }
            }
            
            async convertToWAV(audioBlob) {
                // Simple conversion - in production use a proper audio conversion library
                return new Blob([await audioBlob.arrayBuffer()], { type: 'audio/wav' });
            }
            
            async sendToTranscriptionService(audioBlob) {
                const formData = new FormData();
                formData.append('audio', audioBlob, 'recording.wav');
                
                try {
                    const response = await fetch('/api/transcribe', {
                        method: 'POST',
                        body: formData
                    });
                    
                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(errorData.error || `Server error: ${response.status}`);
                    }
                    
                    const result = await response.json();
                    
                    if (result.analysis) {
                        console.log('üß† Depression Analysis:', result.analysis);
                        
                        // Display analysis results
                        const analysisText = `
Transcript: ${result.transcript}

Depression Analysis:
- Risk Level: ${result.analysis.riskLevel}
- Risk Score: ${result.analysis.riskScore}
- Word Count: ${result.analysis.wordCount}
- Indicators: ${result.analysis.indicators.join(', ')}
                        `;
                        return analysisText.trim();
                    }
                    
                    return result.transcript || 'No speech detected';
                    
                } catch (error) {
                    console.error('Transcription error:', error);
                    throw error;
                }
            }
            
            analyzeTranscript(transcript) {
                // Basic linguistic analysis for depression detection
                const depressiveKeywords = [
                    'sad', 'hopeless', 'tired', 'empty', 'worthless',
                    'anxious', 'worried', 'lonely', 'depressed'
                ];
                
                const positiveKeywords = [
                    'happy', 'excited', 'hopeful', 'energetic',
                    'confident', 'motivated', 'grateful'
                ];
                
                const words = transcript.toLowerCase().split(/\s+/);
                
                const depressiveCount = words.filter(word => 
                    depressiveKeywords.some(keyword => word.includes(keyword))
                ).length;
                
                const positiveCount = words.filter(word => 
                    positiveKeywords.some(keyword => word.includes(keyword))
                ).length;
                
                const analysisResult = {
                    totalWords: words.length,
                    depressiveIndicators: depressiveCount,
                    positiveIndicators: positiveCount,
                    riskScore: this.calculateRiskScore(depressiveCount, positiveCount, words.length)
                };
                
                console.log('Depression Analysis:', analysisResult);
                
                // Here you would send this data to your ML model for further analysis
                return analysisResult;
            }
            
            calculateRiskScore(depressive, positive, total) {
                if (total === 0) return 0;
                
                const depressiveRatio = depressive / total;
                const positiveRatio = positive / total;
                
                // Simple scoring algorithm - replace with your ML model
                return Math.max(0, Math.min(1, (depressiveRatio * 2) - positiveRatio));
            }
            
            updateStatus(type, message) {
                this.statusDisplay.className = `status-display status-${type}`;
                this.statusDisplay.textContent = message;
            }
        }
        
        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', () => {
            new RealTimeAudioRecorder();
        });
    </script>
</body>
</html>