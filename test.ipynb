{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f501acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7dc27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000       # max vocab size\n",
    "MAX_SEQUENCE_LENGTH = 300   # max words per input\n",
    "EMBEDDING_DIM = 100         # embedding vector size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76217308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0                                         oh my gosh   Anxiety\n",
      "1  trouble sleeping, confused mind, restless hear...   Anxiety\n",
      "2  All wrong, back off dear, forward doubt. Stay ...   Anxiety\n",
      "3  I've shifted my focus to something else but I'...   Anxiety\n",
      "4  I'm restless and restless, it's been a month n...   Anxiety\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Combined Data.csv\")  # <-- your CSV file\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69e47e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)   # remove urls\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text) # keep only letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7beb5318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Anxiety' 'Bipolar' 'Depression' 'Normal' 'Personality disorder' 'Stress'\n",
      " 'Suicidal' nan]\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a19033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(df[\"label\"], num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ba9c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df[\"clean_text\"], y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cde01489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bd0cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train = tokenizer.texts_to_sequences(X_train)\n",
    "seq_val = tokenizer.texts_to_sequences(X_val)\n",
    "seq_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(seq_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_val_pad = pad_sequences(seq_val, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_test_pad = pad_sequences(seq_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "659e1b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NItesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m84,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m520\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,093,256</span> (7.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,093,256\u001b[0m (7.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,093,256</span> (7.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,093,256\u001b[0m (7.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=MAX_NUM_WORDS, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    name=\"embedding\"))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3), \n",
    "                       name=\"bidirectional\"))\n",
    "model.add(Dense(64, activation=\"relu\", name=\"dense\"))\n",
    "model.add(Dropout(0.5, name=\"dropout\"))\n",
    "model.add(Dense(num_classes, activation=\"softmax\", name=\"dense_1\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Build the model by calling it with a sample input shape\n",
    "# This forces Keras to build the model and compute parameter counts\n",
    "model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2c5e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 176ms/step - accuracy: 0.5656 - loss: 1.0779 - val_accuracy: 0.6278 - val_loss: 0.8786\n",
      "Epoch 2/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 183ms/step - accuracy: 0.6603 - loss: 0.8213 - val_accuracy: 0.6931 - val_loss: 0.7487\n",
      "Epoch 3/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 189ms/step - accuracy: 0.7288 - loss: 0.6869 - val_accuracy: 0.7266 - val_loss: 0.6905\n",
      "Epoch 4/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 193ms/step - accuracy: 0.7732 - loss: 0.5930 - val_accuracy: 0.7449 - val_loss: 0.6619\n",
      "Epoch 5/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 200ms/step - accuracy: 0.8100 - loss: 0.5037 - val_accuracy: 0.7611 - val_loss: 0.6646\n",
      "Epoch 6/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 194ms/step - accuracy: 0.8407 - loss: 0.4322 - val_accuracy: 0.7613 - val_loss: 0.6678\n",
      "Epoch 7/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 164ms/step - accuracy: 0.8640 - loss: 0.3764 - val_accuracy: 0.7641 - val_loss: 0.6944\n",
      "Epoch 8/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 174ms/step - accuracy: 0.8859 - loss: 0.3169 - val_accuracy: 0.7626 - val_loss: 0.7353\n",
      "Epoch 9/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 173ms/step - accuracy: 0.9036 - loss: 0.2729 - val_accuracy: 0.7540 - val_loss: 0.7862\n",
      "Epoch 10/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 169ms/step - accuracy: 0.9146 - loss: 0.2406 - val_accuracy: 0.7560 - val_loss: 0.8875\n",
      "Epoch 11/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 194ms/step - accuracy: 0.9255 - loss: 0.2089 - val_accuracy: 0.7513 - val_loss: 0.9769\n",
      "Epoch 12/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 163ms/step - accuracy: 0.9362 - loss: 0.1811 - val_accuracy: 0.7519 - val_loss: 1.0409\n",
      "Epoch 13/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 175ms/step - accuracy: 0.9437 - loss: 0.1640 - val_accuracy: 0.7491 - val_loss: 1.0973\n",
      "Epoch 14/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 175ms/step - accuracy: 0.9482 - loss: 0.1470 - val_accuracy: 0.7498 - val_loss: 1.1935\n",
      "Epoch 15/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 166ms/step - accuracy: 0.9538 - loss: 0.1322 - val_accuracy: 0.7421 - val_loss: 1.2865\n",
      "Epoch 16/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 177ms/step - accuracy: 0.9566 - loss: 0.1239 - val_accuracy: 0.7430 - val_loss: 1.3153\n",
      "Epoch 17/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 175ms/step - accuracy: 0.9595 - loss: 0.1124 - val_accuracy: 0.7434 - val_loss: 1.4295\n",
      "Epoch 18/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 168ms/step - accuracy: 0.9626 - loss: 0.1046 - val_accuracy: 0.7479 - val_loss: 1.5386\n",
      "Epoch 19/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 177ms/step - accuracy: 0.9659 - loss: 0.0962 - val_accuracy: 0.7406 - val_loss: 1.5585\n",
      "Epoch 20/20\n",
      "\u001b[1m1327/1327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 238ms/step - accuracy: 0.9680 - loss: 0.0901 - val_accuracy: 0.7426 - val_loss: 1.7004\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c243972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7482, Test Loss: 1.6591\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\")\n",
    "\n",
    "model.save(\"mental_health_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e3eb17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(r\"C:\\Users\\Nitesh\\OneDrive\\Desktop\\models\\mental_health_model.h5\")\n",
    "def predict_sentiment(text):\n",
    "    text_clean = clean_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "    prob = model.predict(pad)[0]\n",
    "    label_id = np.argmax(prob)\n",
    "    return label_encoder.classes_[label_id], prob[label_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "731087e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted: Normal with probability: 0.8804539\n"
     ]
    }
   ],
   "source": [
    "example = \"I feel like dying and have no hope.\"\n",
    "pred_label, pred_prob = predict_sentiment(example)\n",
    "print(\"Predicted:\", pred_label, \"with probability:\", pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd1d1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"mental_health_model_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"mental_health_model_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c7639e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SImple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd import numpy as np import re import tensorflow as tf from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.utils import to_categorical from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional from tensorflow.keras.callbacks import EarlyStopping MAX_NUM_WORDS = 20000 # max vocab size MAX_SEQUENCE_LENGTH = 300 # max words per input EMBEDDING_DIM = 100 # embedding vector size BATCH_SIZE = 32 EPOCHS = 20 df = pd.read_csv(\"Combined Data.csv\") # <-- your CSV file print(df.head()) def clean_text(text): text = str(text).lower() text = re.sub(r\"http\\S+\", \"\", text) # remove urls text = re.sub(r\"[^a-z\\s]\", \" \", text) # keep only letters text = re.sub(r\"\\s+\", \" \", text).strip() return text df[\"clean_text\"] = df[\"text\"].apply(clean_text) label_encoder = LabelEncoder() df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"]) num_classes = len(label_encoder.classes_) print(\"Classes:\", label_encoder.classes_) y = to_categorical(df[\"label\"], num_classes=num_classes) X_train, X_temp, y_train, y_temp = train_test_split( df[\"clean_text\"], y, test_size=0.2, stratify=y, random_state=42 ) X_val, X_test, y_val, y_test = train_test_split( X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42 ) tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token=\"<OOV>\") seq_train = tokenizer.texts_to_sequences(X_train) seq_val = tokenizer.texts_to_sequences(X_val) seq_test = tokenizer.texts_to_sequences(X_test) X_train_pad = pad_sequences(seq_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\") X_val_pad = pad_sequences(seq_val, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\") X_test_pad = pad_sequences(seq_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")tokenizer.fit_on_texts(X_train) model = Sequential() model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name=\"embedding\")) model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3), name=\"bidirectional\")) model.add(Dense(64, activation=\"relu\", name=\"dense\")) model.add(Dropout(0.5, name=\"dropout\")) model.add(Dense(num_classes, activation=\"softmax\", name=\"dense_1\")) model.compile( loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"] ) # Build the model by calling it with a sample input shape # This forces Keras to build the model and compute parameter counts model.build(input_shape=(None, MAX_SEQUENCE_LENGTH)) print(model.summary()) early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True) history = model.fit( X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stop], verbose=1 ) loss, acc = model.evaluate(X_test_pad, y_test, verbose=0) print(f\"Test Accuracy: {acc:.4f}, Test Loss: {loss:.4f}\") model.save(\"mental_health_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
